{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d5dbca-7c44-4fff-ae75-000daa137e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: Nick Nikolov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4600, 57)\n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "(4600,)\n",
      "int64\n",
      "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
      "0               0.21               0.28           0.50           0.0   \n",
      "1               0.06               0.00           0.71           0.0   \n",
      "2               0.00               0.00           0.00           0.0   \n",
      "3               0.00               0.00           0.00           0.0   \n",
      "4               0.00               0.00           0.00           0.0   \n",
      "...              ...                ...            ...           ...   \n",
      "4595            0.31               0.00           0.62           0.0   \n",
      "4596            0.00               0.00           0.00           0.0   \n",
      "4597            0.30               0.00           0.30           0.0   \n",
      "4598            0.96               0.00           0.00           0.0   \n",
      "4599            0.00               0.00           0.65           0.0   \n",
      "\n",
      "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
      "0              0.14            0.28              0.21                0.07   \n",
      "1              1.23            0.19              0.19                0.12   \n",
      "2              0.63            0.00              0.31                0.63   \n",
      "3              0.63            0.00              0.31                0.63   \n",
      "4              1.85            0.00              0.00                1.85   \n",
      "...             ...             ...               ...                 ...   \n",
      "4595           0.00            0.31              0.00                0.00   \n",
      "4596           0.00            0.00              0.00                0.00   \n",
      "4597           0.00            0.00              0.00                0.00   \n",
      "4598           0.32            0.00              0.00                0.00   \n",
      "4599           0.00            0.00              0.00                0.00   \n",
      "\n",
      "      word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
      "0                0.00            0.94  ...                   0.0        0.000   \n",
      "1                0.64            0.25  ...                   0.0        0.010   \n",
      "2                0.31            0.63  ...                   0.0        0.000   \n",
      "3                0.31            0.63  ...                   0.0        0.000   \n",
      "4                0.00            0.00  ...                   0.0        0.000   \n",
      "...               ...             ...  ...                   ...          ...   \n",
      "4595             0.00            0.00  ...                   0.0        0.000   \n",
      "4596             0.00            0.00  ...                   0.0        0.000   \n",
      "4597             0.00            0.00  ...                   0.0        0.102   \n",
      "4598             0.00            0.00  ...                   0.0        0.000   \n",
      "4599             0.00            0.00  ...                   0.0        0.000   \n",
      "\n",
      "      char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
      "0           0.132          0.0        0.372        0.180        0.048   \n",
      "1           0.143          0.0        0.276        0.184        0.010   \n",
      "2           0.137          0.0        0.137        0.000        0.000   \n",
      "3           0.135          0.0        0.135        0.000        0.000   \n",
      "4           0.223          0.0        0.000        0.000        0.000   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "4595        0.232          0.0        0.000        0.000        0.000   \n",
      "4596        0.000          0.0        0.353        0.000        0.000   \n",
      "4597        0.718          0.0        0.000        0.000        0.000   \n",
      "4598        0.057          0.0        0.000        0.000        0.000   \n",
      "4599        0.000          0.0        0.125        0.000        0.000   \n",
      "\n",
      "      capital_run_length_average  capital_run_length_longest  \\\n",
      "0                          5.114                         101   \n",
      "1                          9.821                         485   \n",
      "2                          3.537                          40   \n",
      "3                          3.537                          40   \n",
      "4                          3.000                          15   \n",
      "...                          ...                         ...   \n",
      "4595                       1.142                           3   \n",
      "4596                       1.555                           4   \n",
      "4597                       1.404                           6   \n",
      "4598                       1.147                           5   \n",
      "4599                       1.250                           5   \n",
      "\n",
      "      capital_run_length_total  \n",
      "0                         1028  \n",
      "1                         2259  \n",
      "2                          191  \n",
      "3                          191  \n",
      "4                           54  \n",
      "...                        ...  \n",
      "4595                        88  \n",
      "4596                        14  \n",
      "4597                       118  \n",
      "4598                        78  \n",
      "4599                        40  \n",
      "\n",
      "[4600 rows x 57 columns]\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "4595    0\n",
      "4596    0\n",
      "4597    0\n",
      "4598    0\n",
      "4599    0\n",
      "Name: is_spam, Length: 4600, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "# TO DO: Print size and type of X and y\n",
    "X, y = load_spam()\n",
    "\n",
    "print(X.shape)\n",
    "print(X.dtypes)\n",
    "print(y.shape)\n",
    "print(y.dtypes)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make                0\n",
      "word_freq_labs                0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "word_freq_telnet              0\n",
      "word_freq_lab                 0\n",
      "word_freq_address             0\n",
      "word_freq_650                 0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "capital_run_length_total      0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "nulls = X.isnull().sum().sort_values(ascending=False)\n",
    "print(nulls)\n",
    "\n",
    "nulls = y.isnull().sum()\n",
    "print(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230, 57)\n",
      "(230,)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_small, y_train, y_small = train_test_split(X, y, test_size=0.05, random_state=0)\n",
    "print(X_small.shape)\n",
    "print(y_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ee8a3-2d05-4edf-ba84-4a9879c1cd7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04cca553-3aa8-4d29-9d1e-41b40b3a0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "#Using full X and y dataset\n",
    "full_data = LogisticRegression(max_iter=2000)\n",
    "full_data.fit(X, y)\n",
    "\n",
    "#Using only 5% of the data\n",
    "small_data = LogisticRegression(max_iter=2000)\n",
    "small_data.fit(X_small, y_small)\n",
    "\n",
    "#Using the first two columns of X and y\n",
    "first2 = LogisticRegression(max_iter=2000)\n",
    "first2.fit(X[['word_freq_make', 'word_freq_address']], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "579083a4-1436-484e-beda-03d437227593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score= 0.931\n",
      "validation_score= 0.912\n",
      "train_score= 0.942\n",
      "validation_score= 0.904\n",
      "train_score= 0.616\n",
      "validation_score= 0.610\n",
      "{'fit_time': array([0.00316906, 0.00250173, 0.0022738 , 0.00269485, 0.00256491]), 'score_time': array([0.00074887, 0.00069809, 0.00068712, 0.00077701, 0.00071692]), 'test_score': array([0.60543478, 0.61195652, 0.60217391, 0.61630435, 0.61304348]), 'train_score': array([0.6201087 , 0.61304348, 0.61929348, 0.60842391, 0.62038043])}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score= 0.931\n",
      "validation_score= 0.912\n",
      "train_score= 0.942\n",
      "validation_score= 0.904\n",
      "train_score= 0.616\n",
      "validation_score= 0.610\n",
      "   Data Size  Training Accuracy  Validation Accuracy\n",
      "0   262200.0           0.930870             0.911739\n",
      "1    13110.0           0.942391             0.904348\n",
      "2     9200.0           0.616250             0.609783\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "results = pd.DataFrame(columns=['Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores_full = cross_validate(full_data, X, y, cv=5, scoring='accuracy', return_train_score=True)\n",
    "row = [X.size]\n",
    "\n",
    "for label_pair in [ ('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
    "    print('{}= {:.3f}'.format(label_pair[1], scores_full[label_pair[0]].mean()))\n",
    "    row.append(scores_full[label_pair[0]].mean())\n",
    "\n",
    "results.loc[len(results.index)] = row\n",
    "    \n",
    "    \n",
    "scores_small = cross_validate(small_data, X_small, y_small, cv=5, scoring='accuracy', return_train_score=True)\n",
    "row = [X_small.size]\n",
    "    \n",
    "for label_pair in [ ('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
    "    print('{}= {:.3f}'.format(label_pair[1], scores_small[label_pair[0]].mean()))\n",
    "    row.append(scores_small[label_pair[0]].mean())\n",
    "results.loc[len(results.index)] = row\n",
    "\n",
    "    \n",
    "scores_first2 = cross_validate(first2, X[['word_freq_make', 'word_freq_address']], y, cv=5, scoring='accuracy', return_train_score=True)\n",
    "row = [X[['word_freq_make', 'word_freq_address']].size]\n",
    "    \n",
    "for label_pair in [ ('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
    "    print('{}= {:.3f}'.format(label_pair[1], scores_first2[label_pair[0]].mean()))\n",
    "    row.append(scores_first2[label_pair[0]].mean())\n",
    "results.loc[len(results.index)] = row\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2ac5cad-73d7-42a7-bdda-68b11777ad7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. Using only 5% of the data produced basically the same results in terms of training and validation accuracy. +/- 1%. Training accuracy improved by 1% after decreasing the data size, while validation accuracy only dropped by 1%. If the data set was large I think this drop in validation accuracy could be acceptable to same compute time. \n",
    "\n",
    "2. The dataset is used to represent a sample of emails and is used to detect spam emails to filter out. A false positive would represent marking non-spam email as spam. A false negative is allowing a spam email into the inbox. I think which being worse is debatable. If a spam email is accidentally placed into the inbox, if it is an effective phishing email it might cause a lot of damage to the receptient. However, if I receive a job offer and it accidentally gets marked as spam then I may never see it! Personally, I would prefer more false negatives since I trust myself to filter out phishing emails myself. But if I was able to setup my grandma's email spam filter, I would be as restrictive as possible to make sure she does not get phished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd0728-495a-4c6e-a14a-4c3217c37608",
   "metadata": {},
   "source": [
    "1. I used the sci-kit-learn documentation website to learn about the regression function parameters. I also used lecture & lab slides created by Dr. Dawson.\n",
    "2. I completed all steps in numerical order.\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed02275-ebc8-4026-a141-046d97890a6c",
   "metadata": {},
   "source": [
    "1. I used the sci-kit-learn documentation website to learn about the regression function parameters. I also used lecture & lab slides created by Dr. Dawson.\n",
    "\n",
    "2. I completed all steps in numerical order.\n",
    "\n",
    "3. Any time I was stuck on a compilation error I pasted the error message into ChatGPT. If it provided an easy fix, I modified my code accordingly and retested. My prompt was always identical to the error message I received. \n",
    "\n",
    "4. Yes I had some challenges with setting up the training and testing data sets. The assignment indicates we need to split the dataset into a smaller dataset containing 5% of the original data. However, there was no further guidance on what data to use for the training / testing of each model. Should we have used 5% of the dataset to train / test all models or only the model that specified 5%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a6e9d-480f-4f65-8bf7-92894ed427bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47623d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c827b-7b49-44f9-ab69-3a246d47e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citations\n",
    "1. Cranor, Lorrie Faith, and Brian A. LaMacchia. “Spam!.” Communications of the ACM 41.8 (1998): 74-83.\n",
    "2. Dr. Dawson - Lecture slides\n",
    "3. https://scikit-learn.org/stable/\n",
    "4. https://chat.openai.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
